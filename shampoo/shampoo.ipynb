{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 7824421.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 9371475.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 6604035.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4557542.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0), (1))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Iterable\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class BasicNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 128, bias=False),\n",
    "            nn.GELU(),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 64, bias=False),\n",
    "            nn.GELU(),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 10, bias=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Claude's function\n",
    "def matrix_power_neg_quarter(A, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Compute A^(-1/4) for a symmetric positive definite matrix A.\n",
    "    \"\"\"\n",
    "    # Compute eigendecomposition\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(A)\n",
    "    \n",
    "    # Compute powered eigenvalues\n",
    "    powered_eigenvalues = 1.0 / torch.pow(torch.clamp(eigenvalues, min=epsilon), 0.25)\n",
    "    \n",
    "    # Reconstruct the matrix\n",
    "    return eigenvectors @ torch.diag(powered_eigenvalues) @ eigenvectors.t()\n",
    "\n",
    "class ShampooOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, parameters: Iterable[Parameter], lr=0.001, betas=(0.9, 0.999), epsilon=1e-8):\n",
    "        self.params = list(parameters)\n",
    "        self.lr = lr\n",
    "        defaults = dict(lr=lr, betas=betas, epsilon=epsilon)\n",
    "        super(ShampooOptimizer, self).__init__(self.params, defaults)\n",
    "        \n",
    "        # assumes 2D everything. Don't want to even think about 3D\n",
    "\n",
    "        # self.G_hat = [torch.zeros(p.shape) for p in self.params] # d1 x d2\n",
    "        self.L = [epsilon * torch.eye(int(p.shape[0])) for p in self.params] # d1 x d1\n",
    "        # self.L_tilda = [torch.zeros((p.shape[0], p.shape[0])) for p in self.params] # d1 x d1\n",
    "        \n",
    "        self.R = [epsilon * torch.eye(int(p.shape[1])) for p in self.params] # d2 x d2\n",
    "        # self.R_tilda = [torch.zeros((p.shape[1], p.shape[1])) for p in self.params] # d2 x d2\n",
    "        \n",
    "        # self.M = [epsilon * torch.zeros(p.shape) for p in self.params] # d1 x d2\n",
    "        \n",
    "    def step(self, closure = None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "            \n",
    "    \n",
    "        # I'm treating weights and bias seperately \n",
    "        L = self.L\n",
    "        R = self.R\n",
    "        for i, param in enumerate(self.params):\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "            L[i].add_(torch.matmul(param.grad, param.grad.T))\n",
    "            R[i].add_(torch.matmul(param.grad.T, param.grad))\n",
    "            \n",
    "            D = torch.matmul(torch.matmul(matrix_power_neg_quarter(L[i]), param.grad), matrix_power_neg_quarter(R[i]))\n",
    "            \n",
    "            param.data.sub_(D, alpha=self.lr)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss at index 100: 1.7933749449253082 | took 12.877652406692505 seconds\n",
      "average loss at index 200: 1.5970934927463531 | took 11.907608985900879 seconds\n",
      "average loss at index 300: 1.5768053781986238 | took 11.973227739334106 seconds\n",
      "average loss at index 400: 1.5609676575660705 | took 12.131686687469482 seconds\n",
      "average loss at index 500: 1.5553697979450225 | took 12.106920957565308 seconds\n",
      "average loss at index 600: 1.552356003522873 | took 12.12163758277893 seconds\n",
      "average loss at index 700: 1.5471753704547881 | took 12.13194990158081 seconds\n",
      "average loss at index 800: 1.5423400354385377 | took 12.200146436691284 seconds\n",
      "average loss at index 900: 1.5334815895557403 | took 12.112584829330444 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model = BasicNN()\n",
    "loss_fn = F.cross_entropy\n",
    "optimizer = ShampooOptimizer(model.parameters(), lr=0.1)\n",
    "\n",
    "total_loss = 0\n",
    "last_time = time.time()\n",
    "for idx, (x, label) in enumerate(train_loader):\n",
    "    batch_size = x.shape[0]\n",
    "    logits = model(x.view(batch_size, -1))\n",
    "    logits = F.softmax(logits, dim=-1)\n",
    "\n",
    "    loss = loss_fn(logits, label)\n",
    "    total_loss += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if idx % 100 == 0 and idx != 0:\n",
    "        print(f\"average loss at index {idx}: {total_loss/100} | took {time.time()-last_time} seconds\")\n",
    "        last_time = time.time()\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total accuracy: 0.9404000043869019\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (img, label) in enumerate(test_loader):\n",
    "        batch_size = img.shape[0]\n",
    "        logits = model(img.view(batch_size, -1))\n",
    "        logits = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        correct += (logits.argmax(dim=-1) == label).sum()\n",
    "        total += label.numel()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"total accuracy: {correct/total}\")\n",
    "# It does poorly because of improper backward pass implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.optim.Adam([torch.randn(3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megatron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
